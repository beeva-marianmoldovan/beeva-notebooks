{
 "metadata": {
  "name": "",
  "signature": "sha256:a4fa1796886ecdae9dd61e759f0553d3a5050c376c0f267d77e582bc6de36426"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Preparar limpieza de datos"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Descargar datos con el script: extraer_es.sh (se puede encontrar en https://github.com/beeva-nievesabalos/scripts/tree/master/wikipedia-es)\n",
      "\n",
      "Obtenemos el siguiente archivo con los datos: **data/wikipedia_es.xml**\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re, unicodedata, logging, string\n",
      "from gensim import models\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Ejemplo: wikipedia_es.xml"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Leo los datos"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fileToRead = \"/home/nievesabalos/Escritorio/nlp/datasets/wikipedia/data/wikipedia_es_small.xml\"\n",
      "# ---------------------------------------------------\n",
      "# ---------------------------------------------------\n",
      "# 1. INPUT DATA\n",
      "print \"Fichero:\"\n",
      "print fileToRead\n",
      "\n",
      "def clean(x):\n",
      "   x = unicodedata.normalize('NFKD', x).encode('ascii','ignore').lower()\n",
      "   replace_punctuation = string.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
      "   x = x.translate(replace_punctuation)\n",
      "   x = re.sub('@%$&[\\n/:!,;)()_?\u00bf\u00a1<>]', ' ', x)\n",
      "   x = re.sub(' - ', ' ', x)\n",
      "   x = re.sub(' +',' ', x).strip()\n",
      "   return x\n",
      "    \n",
      "sentences = []\n",
      "with open(fileToRead, 'r') as fileData:\n",
      "    for lineas in fileData:\n",
      "        #Formatear linea si hace falta aqu\u00ed\n",
      "        lineArray = lineas.split(\".\")\n",
      "        for line in lineArray:\n",
      "            if len(line) > 1:\n",
      "                line = line.decode('utf-8')\n",
      "                line = clean(line) # \u00bf? problemas con gensim y tildes y e\u00f1es...\n",
      "                if len(line) > 1:\n",
      "                    sentences.append(line.split(\" \"))\n",
      "       \n",
      "    print \"Cierro fichero.\"\n",
      "    fileData.close()\n",
      "\n",
      "\n",
      "print \"\\nFrases a analizar con word2vec:\"             \n",
      "print len(sentences)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fichero:\n",
        "/home/nievesabalos/Escritorio/nlp/datasets/wikipedia/data/wikipedia_es_small.xml\n",
        "Cierro fichero."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Frases a analizar con word2vec:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4678492\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Voy a crear el modelo con las palabras obtenidas de la wikipedia"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sentences[0:30]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['doc', 'id', '351623', 'url', 'http', 'es'], ['wikipedia'], ['org', 'wiki', 'curid', '351623', 'title', 'senal', 'abierta'], ['senal', 'abierta'], ['una', 'senal', 'abierta', 'es', 'una', 'senal', 'de', 'radio', 'o', 'television', 'que', 'se', 'transmite', 'sin', 'cifrar', 'y', 'pueden', 'ser', 'recibidas', 'a', 'traves', 'de', 'cualquier', 'receptor', 'adecuado'], ['las', 'emisiones', 'free', 'to', 'view', 'libre', 'para', 'ver', 'o', 'ftv', 'estan', 'por', 'lo', 'general', 'disponibles', 'sin', 'suscripcion', 'pero', 'son', 'cifradas', 'digitalmente', 'y', 'su', 'acceso', 'puede', 'ser', 'restringido', 'a', 'cierta', 'area', 'geografica'], ['estas', 'emisiones', 'son', 'distintas', 'a', 'las', 'usadas', 'para', 'ofrecer', 'servicios', 'de', 'television', 'por', 'pago', 'o', 'pago', 'por', 'vision'], ['las', 'senales', 'abiertas', 'pueden', 'estar', 'disponibles', 'en', 'algunos', 'paises', 'a', 'traves', 'de', 'difusion', 'directa', 'por', 'satelite', 'mas', 'en', 'muchas', 'partes', 'del', 'mundo', 'los', 'canales', 'emiten', 'su', 'senal', 'sin', 'cifrar', 'usando', 'las', 'bandas', 'vhf', 'y', 'uhf'], ['a', 'pesar', 'de', 'que', 'estos', 'canales', 'son', 'de', 'acceso', 'libre', 'en', 'algunos', 'casos', 'el', 'publico', 'paga', 'sus', 'costes', 'de', 'operacion'], ['algunos', 'de', 'ellos', 'son', 'sufragados', 'directamente', 'por', 'el', 'pago', 'de', 'impuestos', 'generales', 'un', 'canon', 'televisivo', 'como', 'en', 'el', 'caso', 'de', 'la', 'bbc', 'en', 'el', 'reino', 'unido', 'o', 'la', 'donacion', 'voluntaria', 'en', 'el', 'caso', 'de', 'los', 'organismos', 'de', 'radiodifusion', 'con', 'fines', 'educativos', 'y', 'culturales', 'como', 'la', 'norteamericana', 'pbs'], ['ademas', 'algunas', 'senales', 'abiertas', 'tienen', 'publicidad', 'comercial'], ['a', 'menudo', 'se', 'utilizan', 'las', 'senales', 'abiertas', 'para', 'radiodifusion', 'internacional', 'haciendole', 'un', 'equivalente', 'en', 'video', 'a', 'la', 'radio', 'de', 'onda', 'corta'], ['codificacion'], ['en', 'la', 'actualidad', 'se', 'reciben', 'senales', 'fta', 'casi', 'de', 'todo', 'el', 'mundo', 'norteamerica', 'centroamerica', 'sudamerica', 'europa', 'asia', 'y', 'en', 'menor', 'medida', 'de', 'africa', 'y', 'algunas', 'islas', 'del', 'caribe'], ['las', 'senales', 'fta', 'vienen', 'codificadas', 'en', 'norma', 'dvb', 's', 'con', 'video', 'y', 'audio', 'comprimido', 'con', 'el', 'codec', 'mpeg2', 'y', 'senal', 'de', 'video', 'en', 'banda', 'base', 'en', 'formato', 'ntsc', 'm', 'o', 'pal', 'b'], ['actualmente', 'se', 'esta', 'migrando', 'al', 'codec', 'mpeg4', 'lo', 'que', 'requiere', 'de', 'receptores', 'de', 'dvb', 's2', 'norma', 'que', 'admite', 'este', 'nuevo', 'codec'], ['las', 'senales', 'de', 'fta', 'se', 'suelen', 'transmitir', 'en', 'resolucion', 'estandar', 'sd', 'de', 'acuerdo', 'a', 'las', 'distintas', 'normas', 'de', 'tv', 'en', 'uso', 'en', 'el', 'mundo', 'aunque', 'en', 'ciertos', 'lugares', 'ya', 'hay', 'senales', 'fta', 'en', 'alta', 'definicion', 'hd', 'estas', 'requieren', 'de', 'un', 'receptor', 'satelital', 'dvb', 's2', 'con', 'capacidad', 'hd', 'y', 'un', 'televisor', 'hd'], ['incluso', 'algunas', 'tienen', 'sonido', 'dolby', 'digital'], ['la', 'transmision', 'de', 'senales', 'se', 'realiza', 'preferentemente', 'a', 'traves', 'de', 'las', 'bandas', 'satelitales', 'ku', 'y', 'c'], ['la', 'mayoria', 'de', 'las', 'senales', 'son', 'transmitidas', 'usando', 'satelites', 'estadounidenses'], ['existen', 'pocas', 'emisoras', 'canadienses', 'que', 'transmiten', 'en', 'banda', 'ku', 'que', 'de', 'verdad', 'utilizan', 'largura', 'de', 'banda', 'no', 'utilizada', 'por', 'los', 'canales', 'de', 'pago', 'dificultando', 'asi', 'la', 'sintonia', 'de', 'canales', 'fta'], ['las', 'senales', 'de', 'fta', 'pueden', 'ademas', 'ser', 'emitidas', 'por', 'varios', 'satelites', 'siendo', 'necesario', 'la', 'utilizacion', 'de', 'un', 'motor', 'para', 'permitir', 'que', 'la', 'antena', 'cambie', 'de', 'posicion', 'para', 'sintonizarlos', 'o', 'aumentar', 'la', 'cantidad', 'de', 'lnbs', 'para', 'recepcionarlos', 'todos'], ['algunos', 'ejemplos', 'de', 'canales', 'y', 'paises', 'que', 'trasmiten', 'fta', 'y', 'que', 'pueden', 'ser', 'captados', 'con', 'un', 'receptor', 'de', 'satelite', 'estandar', 'en', 'muchas', 'partes', 'de', 'america', 'y', 'europa'], ['es', 'solo', 'por', 'nombrar', 'algunos', 'pues', 'tenemos', 'canales', 'de', 'paises', 'como', 'portugal', 'francia', 'italia', 'peru', 'paraguay', 'uruguay', 'mexico', 'brasil', 'ecuador', 'paises', 'arabes', 'china', 'japon', 'holanda', 'bulgaria', 'corea', 'honduras', 'el', 'salvador', 'guatemala', 'entre', 'otros'], ['en', 'uruguay', 'esta', 'terminantemente', 'prohibido', 'la', 'recepcion', 'de', 'estos', 'canales', 'ya', 'que', 'una', 'resolucion', 'presentada', 'por', 'los', 'operadores', 'de', 'cable', 'y', 'aprobada', 'por', 'el', 'gobierno', 'impide', 'la', 'recepcion', 'y', 'comercializacion', 'de', 'receptores', 'satelitales'], ['obsolescencia', 'de', 'los', 'receptores', 'de', 'fta'], ['muchos', 'receptores', 'podrian', 'tener', 'opciones', 'de', 'expansion', 'de', 'hardware', 'asi', 'como', 'anadir', 'recepcion', '8psk', 'o', 'dvb', 'de', 'interfaz', 'comun', 'para', 'tarjetas', 'de', 'suscripcion', 'tv', 'y', 'un', 'upgrade', 'de', 'mejoramiento', 'del', 'firmware', 'cualquiera', 'de', 'las', 'oficiales', 'o', 'de', 'las', 'nominalmente', 'fuentes', 'de', 'terceras', 'partes'], ['es', 'muy', 'frecuente', 'que', 'una', 'vez', 'que', 'los', 'receptores', 'individuales', 'son', 'descontinuados', 'el', 'soporte', 'y', 'la', 'expansion', 'desaparece', 'rapidamente', 'de', 'las', 'fuentes'], ['la', 'migracion', 'existente', 'alimenta', 'los', 'formatos', 'mpeg', '4', 'hdtv', 'o', 'dvb', 's2', 'los', 'cuales', 'muchos', 'receptores', 'normales', 'no', 'soportan', 'ademas', 'puede', 'que', 'los', 'telespectadores', 'pierdan', 'la', 'programacion', 'libre', 'como', 'a', 'la', 'vez', 'los', 'equipos', 'que', 'rapidamente', 'quedan', 'obsoletos'], ['muy', 'diferente', 'a', 'los', 'set', 'top', 'boxes', 'digitales', 'terrestres', 'los', 'estandares', 'de', 'definicion', 'dvb', 's', 'de', 'estos', 'receptores', 'no', 'bajan', 'ni', 'convierten', 'programacion', 'en', 'hd', 'asi', 'que', 'estos', 'productos', 'no', 'pueden', 'utilizar', 'video', 'para', 'estas', 'senales']]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Creo el modelo"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Creo el modelo con los datos de la wikipedia en castellano"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ---------------------------------------------------\n",
      "# 2. CREATE MODEL\n",
      "# train word2vec \n",
      "# parametros por defecto:\n",
      "# size=100, alpha=0.025, window=5, min_count=5, sample=0, seed=1, workers=1, \n",
      "# min_alpha=0.0001, sg=1, hs=1, negative=0, cbow_mean=0, hashfxn=<built-in function hash>,\n",
      "# iter=1\n",
      "print \"\\nCreo modelo word2vec:\"   \n",
      "model = models.Word2Vec(sentences, size=100, workers=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Creo modelo word2vec:\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Guardo el modelo"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ---------------------------------------------------\n",
      "# 3. SAVE MODEL\n",
      "# Ficheros del modelo en diferentes formatos\n",
      "fileModel = '/home/nievesabalos/Escritorio/nlp/datasets/wikipedia/modelo/es_wikipedia.model'  #.model\n",
      "fileModelBin = '/home/nievesabalos/Escritorio/nlp/datasets/wikipedia/modelo/es_wikipedia.model.bin' #.bin\n",
      "fileModelTxt = '/home/nievesabalos/Escritorio/nlp/datasets/wikipedia/modelo/es_wikipedia.model.txt' #.txt\n",
      "\n",
      "print \"\\nGuardo el modelo en .model:\"  \n",
      "print fileModel \n",
      "model.save(fileModel)\n",
      "print \"\\nGuardo el modelo en .bin:\"  \n",
      "print fileModelBin \n",
      "model.save_word2vec_format(fileModelBin, binary=True)\n",
      "print \"\\n...y guardo el modelo en .txt:\"  \n",
      "print fileModelTxt \n",
      "model.save_word2vec_format(fileModelTxt, binary=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Guardo el modelo en .model:\n",
        "/home/nievesabalos/Escritorio/nlp/datasets/wikipedia/modelo/es_wikipedia.model\n",
        "\n",
        "Guardo el modelo en .bin:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "/home/nievesabalos/Escritorio/nlp/datasets/wikipedia/modelo/es_wikipedia.model.bin\n",
        "\n",
        "...y guardo el modelo en .txt:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "/home/nievesabalos/Escritorio/nlp/datasets/wikipedia/modelo/es_wikipedia.model.txt\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Y analizo los resultados..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "C\u00f3mo se parecen dos palabras entre s\u00ed:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"\\nSimilitud entre:pregunta y novio\"  \n",
      "similitud = model.similarity('pregunta', 'novio')\n",
      "print similitud"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Similitud entre:pregunta y novio\n",
        "0.387129717011"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Qu\u00e9 5 palabras se parecen m\u00e1s a..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"\\nM\u00e1s similiar a: humanidades\"  \n",
      "similitud = model.most_similar([u'humanidades'], topn=5)\n",
      "print similitud\n",
      "print \"\\nM\u00e1s similiar a: alemania\"  \n",
      "similitud = model.most_similar([u'alemania'], topn=5)\n",
      "print similitud\n",
      "print \"\\nM\u00e1s similiar a: novio\"  \n",
      "similitud = model.most_similar([u'novio'], topn=5)\n",
      "print similitud\n",
      "print \"\\nM\u00e1s similiar a: coche\"  \n",
      "similitud = model.most_similar([u'coche'], topn=5)\n",
      "print similitud\n",
      "print \"\\nM\u00e1s similiar a: telecinco\"  \n",
      "similitud = model.most_similar([u'telecinco'], topn=5)\n",
      "print similitud\n",
      "\n",
      "print \"\\nM\u00e1s similiar a: religion\"  \n",
      "similitud = model.most_similar([u'religion'], topn=5)\n",
      "print similitud\n",
      "print \"\\nM\u00e1s similiar a: relaciones\"  \n",
      "similitud = model.most_similar([u'relaciones'], topn=5)\n",
      "print similitud\n",
      "\n",
      "print \"\\nM\u00e1s similiar a: categoria\"  \n",
      "similitud = model.most_similar([u'categoria'], topn=5)\n",
      "print similitud\n",
      "print \"\\nM\u00e1s similiar a: subcategoria\"  \n",
      "similitud = model.most_similar([u'subcategoria'], topn=5)\n",
      "print similitud\n",
      "print \"\\nM\u00e1s similiar a: idioma\"  \n",
      "similitud = model.most_similar([u'idioma'], topn=5)\n",
      "print similitud\n",
      "print \"\\nM\u00e1s similiar a: pregunta\"  \n",
      "similitud = model.most_similar([u'pregunta'], topn=5)\n",
      "print similitud"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "M\u00e1s similiar a: humanidades\n",
        "[('ciencias', 0.8768219351768494), ('biomedicas', 0.83720862865448), ('sociologia', 0.8258548378944397), ('agronomia', 0.8147366046905518), ('fisicomatematicas', 0.814551055431366)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "M\u00e1s similiar a: alemania\n",
        "[('checoslovaquia', 0.8262768983840942), ('finlandia', 0.823238730430603), ('holanda', 0.8193880915641785), ('belgica', 0.7939984798431396), ('polonia', 0.7847803831100464)]\n",
        "\n",
        "M\u00e1s similiar a: novio\n",
        "[('novia', 0.8637279272079468), ('padrastro', 0.787034273147583), ('marido', 0.7670013904571533), ('esposo', 0.7442255020141602), ('amante', 0.7426474690437317)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "M\u00e1s similiar a: coche\n",
        "[('camion', 0.8076907992362976), ('vehiculo', 0.7972133159637451), ('automovil', 0.7833150029182434), ('aeroplano', 0.7595457434654236), ('vagon', 0.7370368242263794)]\n",
        "\n",
        "M\u00e1s similiar a: telecinco\n",
        "[('tve', 0.8757566213607788), ('chilevision', 0.8382378816604614), ('telemadrid', 0.8167874217033386), ('sogecable', 0.8009046316146851), ('tv3', 0.793958306312561)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "M\u00e1s similiar a: religion\n",
        "[('doctrina', 0.7783756852149963), ('siriaca', 0.7305366396903992), ('liturgia', 0.7177563905715942), ('ortodoxia', 0.7064704895019531), ('copta', 0.701934278011322)]\n",
        "\n",
        "M\u00e1s similiar a: relaciones\n",
        "[('disputas', 0.6944965124130249), ('fraternales', 0.6911996603012085), ('vinculaciones', 0.6755918860435486), ('responsabilidades', 0.672494649887085), ('amistosas', 0.6474363803863525)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "M\u00e1s similiar a: categoria\n",
        "[('division', 0.6383973956108093), ('competicion', 0.6026310920715332), ('250cc', 0.5964064598083496), ('liga', 0.5850504040718079), ('liguera', 0.5532511472702026)]\n",
        "\n",
        "M\u00e1s similiar a: subcategoria\n",
        "[('ondulacion', 0.6945493817329407), ('variacion', 0.6704306602478027), ('neurotoxina', 0.6646198034286499), ('divergente', 0.6620479822158813), ('reelaboracion', 0.6616384983062744)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "M\u00e1s similiar a: idioma\n",
        "[('dialecto', 0.8296833038330078), ('sanscrito', 0.800920844078064), ('casubio', 0.7715018391609192), ('indoeuropeo', 0.7595945000648499), ('hnahnu', 0.7562755346298218)]\n",
        "\n",
        "M\u00e1s similiar a: pregunta\n",
        "[('contesta', 0.7927286028862), ('ebern', 0.7645688056945801), ('miente', 0.7643635272979736), ('preguntandole', 0.7446896433830261), ('kugo', 0.7422221899032593)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Qu\u00e9 palabra se parece m\u00e1s a [positive] y no a [negative]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"\\nM\u00e1s similar entre pareja y novio que no se parezca a hombre:\"  \n",
      "mas_similar = model.most_similar(positive=['pareja', 'novio'], negative=['hombre'])\n",
      "print mas_similar\n",
      "\n",
      "print \"\\nM\u00e1s similar entre religion y hombre:\"  \n",
      "mas_similar = model.most_similar(positive=['religion', 'hombre'])\n",
      "print mas_similar\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "M\u00e1s similar entre pareja y novio que no se parezca a hombre:\n",
        "[('novia', 0.7309137582778931), ('companera', 0.7067101001739502), ('esposa', 0.6432494521141052), ('abuela', 0.6324878334999084), ('hermana', 0.6294979453086853), ('madre', 0.6235520243644714), ('cunada', 0.615567684173584), ('patty', 0.6132184267044067), ('heather', 0.5999829173088074), ('jessica', 0.5997812747955322)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "M\u00e1s similar entre religion y hombre:\n",
        "[('creyente', 0.7283495664596558), ('sabiduria', 0.7251049280166626), ('monoteista', 0.7138979434967041), ('vudu', 0.674062967300415), ('egoista', 0.6670193672180176), ('daimon', 0.6668726205825806), ('individualismo', 0.6615554094314575), ('ethos', 0.6602845191955566), ('dios', 0.6599222421646118), ('vicio', 0.6534897685050964)]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El vector que representa una palabra"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"\\nWord vector de pregunta:\"  \n",
      "vector = model['pregunta']  # raw numpy vector of a word\n",
      "print vector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Word vector de pregunta:\n",
        "[-0.19512661  0.16286226 -0.05837421 -0.26061872 -0.01471696 -0.22496428\n",
        " -0.13044791 -0.26371366 -0.1476597   0.48989224  0.08896273  0.30480862\n",
        " -0.53299612 -0.09006413  0.19677725 -0.1478468  -0.17109273  0.05557674\n",
        " -0.51965833  0.26225221 -0.10670206  0.4237093   0.46415117  0.11005138\n",
        "  0.0378735  -0.34525499  0.25790522 -0.20079702  0.24876137  0.11080693\n",
        "  0.23974584  0.34592089 -0.02112323 -0.17107229  0.09610246  0.24444483\n",
        " -0.13187009  0.25379491 -0.10974453 -0.01006108 -0.2921733  -0.31988245\n",
        " -0.21408577  0.17441623 -0.78275454 -0.07582462  0.15140155  0.06978978\n",
        " -0.25411496  0.04708477 -0.2665619  -0.00098227 -0.21440502  0.02947058\n",
        "  0.02664166  0.03584042 -0.01348349 -0.06914473  0.0097377   0.01087987\n",
        " -0.26704043  0.35003805  0.32461622 -0.01993486  0.04961406  0.19362417\n",
        " -0.35397828 -0.28204817  0.21183772 -0.28572872  0.11325006 -0.1184326\n",
        "  0.18753098 -0.16289777 -0.43160975 -0.40098867 -0.57134259 -0.24340439\n",
        "  0.36849698 -0.43697888  0.23516665  0.33388826 -0.08740797  0.00593617\n",
        "  0.27175364  0.21373239  0.03985664  0.04842515 -0.06198032 -0.18979533\n",
        " -0.14439589  0.15725704  0.01907291 -0.48778656  0.27732515  0.08863875\n",
        "  0.18731423  0.27513412  0.39428374 -0.24394558]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Fin"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc.stop()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}