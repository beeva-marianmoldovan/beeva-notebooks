{
 "metadata": {
  "name": "",
  "signature": "sha256:4fc283b91c90801998fad3e4cabb4c4b76580ebea8e17af363b6f2fe10924ed2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# SocialProfiler\n",
      "\n",
      "*Analyzing user interests based on Instagram texts*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Web scraping"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from BeautifulSoup import BeautifulSoup\n",
      "from time import sleep\n",
      "\n",
      "def get_urlpics(driver):\n",
      "    soup = BeautifulSoup(driver.page_source)\n",
      "    urlpics = map(lambda x: x['src'], soup.findAll('div', {'class':'pgmiThumb tThumbImage Image'}))\n",
      "    return urlpics\n",
      "\n",
      "def get_texts(url,driver, processedurls):\n",
      "    texts = []\n",
      "    soup = BeautifulSoup(driver.page_source)\n",
      "    links = map(lambda x: x['href'], soup.findAll('a', {'class':'pgmiImageLink'}))\n",
      "    newlinks = [link for link in links if link not in processedurls]\n",
      "    processedurls.extend(newlinks)\n",
      "    \n",
      "    texts = [get_text_from_url(driver,url+link) for link in newlinks]\n",
      "    return (texts, processedurls)\n",
      "\n",
      "def get_text_from_url(driver, link):\n",
      "    driver.get(link)\n",
      "    sleep(2)\n",
      "    try:\n",
      "        text = driver.find_element_by_css_selector('h1.sCaption').text\n",
      "        #text = driver.find_element_by_css_selector('div.Info').text\n",
      "    except:\n",
      "        text = \"\"\n",
      "    return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import selenium.webdriver as webdriver\n",
      "\n",
      "def browser_user_instagram(user):\n",
      "    url = 'http://instagram.com/'+user+'/'\n",
      "    driver = webdriver.Firefox()\n",
      "    driver.get(url)\n",
      "    driver.find_element_by_css_selector('.pgmiThumb')\n",
      "    not_all_images_loaded = True\n",
      "    \n",
      "    processedurls = []\n",
      "    \n",
      "    while not_all_images_loaded:\n",
      "        \n",
      "        try:\n",
      "            driver.find_element_by_class_name('PhotoGridMoreButton').click()\n",
      "            sleep(1)\n",
      "            webElement = driver.find_element_by_css_selector('.PhotoGridMoreButton').text\n",
      "            (texts, processedurls) = get_texts(url,driver, processedurls)\n",
      "           \n",
      "            #driver.find_element_by_css_selector('.igDialogClose').click()\n",
      "            if (webElement == \"All items loaded\") or (webElement == \"Se han subido todos los elementos\") or (len(texts) >= LIMIT):\n",
      "                not_all_images_loaded = False\n",
      "                print webElement\n",
      "            driver.quit()\n",
      "        except:\n",
      "            pass\n",
      "    return texts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LIMIT = 10\n",
      "user = 'giseleofficial'\n",
      "texts = browser_user_instagram(user)\n",
      "print \"%i texts processed\" %len(texts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Load more\u2026\n",
        "40 texts processed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Text analytics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "def tokenize(text):\n",
      "    return text\n",
      "\n",
      "vocabulary = ['food','family','animals','nature','fashion','love','tourism']\n",
      "\n",
      "vectorizer = TfidfVectorizer(analyzer=u'word', binary=False, charset=None, tokenizer=None, use_idf=True, vocabulary=vocabulary)\n",
      "tfidf = vectorizer.fit_transform(texts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/enrique/herramientas/anaconda/lib/python2.7/site-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_keywords_by_idx(idx):\n",
      "    tfidf_feature_names = vectorizer.get_feature_names()\n",
      "    #Let\u2019s convert the matrix to dense format to explore further\n",
      "    dense = tfidf.todense()\n",
      "\n",
      "    mydoc = dense[idx].tolist()[0]\n",
      "    phrase_scores = [pair for pair in zip(range(0, len(mydoc)), mydoc) if pair[1] > 0]\n",
      "    \n",
      "    # we\u2019ll sort the phrases by score in descending order to find the most interesting phrases \n",
      "    # The first value in each tuple is the phrase\u2019s position in our initial vector and also corresponds \n",
      "    # to the phrase\u2019s position in feature_names which allows us to map the scores back to phrases. \n",
      "    sorted_phrase_scores = sorted(phrase_scores, key=lambda t: t[1] * -1)\n",
      "    keywords = []\n",
      "    for phrase, score in [(tfidf_feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:20]:\n",
      "        keywords.append('{0: <20} {1}'.format(phrase.encode('utf-8'), score))\n",
      "    return keywords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "def get_topic_by_idx(idx):\n",
      "    result = None\n",
      "    try:\n",
      "        tfidf_feature_names = vectorizer.get_feature_names()\n",
      "        densematrix = tfidf[idx].todense()\n",
      "        idx_vocab = np.where(np.array(densematrix != 0)[0])[0][0]\n",
      "        result = tfidf_feature_names[idx_vocab]\n",
      "    except:\n",
      "        pass\n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "idxsample = np.where(np.array(map(lambda x: [x.find(word) for word in ['nature']], texts)) > 0)[0]\n",
      "for idx in idxsample:\n",
      "    print \"idx: %i\" %idx\n",
      "    print \"text: %s\" %texts[idx]\n",
      "    print \"topics: %s\" %get_keywords_by_idx(idx)\n",
      "    print \"topic: %s\" %get_topic_by_idx(idx)\n",
      "    print \"\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "idx: 22\n",
        "text: #inspiration #love #nature \ud83c\udf38\ud83c\udf38\ud83c\udf38 #inspira\u00e7\u00e3o #amor #natureza\n",
        "topics: ['nature               0.847658710581', 'love                 0.530541902564']\n",
        "topic: nature\n",
        "\n",
        "\n",
        "idx: 34\n",
        "text: Forests are vital to life on Earth. They purify the air we breathe, filter the water we drink, offer a home to much of the world\u2019s diverse array of plants and animals. And even doing all those things and much more for us, it is being destroyed at a rate of 48 football fields per minute. Awareness is the first step to really making a change. I support #HUGATREE #lovenature @world_wildlife \ud83c\udf33\ud83c\udf33\ud83c\udf33 As florestas s\u00e3o vitais para a vida na terra. Elas purificam o ar que respiramos, filtram a \u00e1gua que bebemos, oferecem um lar para grande parte das plantas e animais. E mesmo fazendo tudo isso e muito mais para n\u00f3s, elas est\u00e3o sendo destru\u00eddas a uma taxa de 48 campos de futebol por minuto. A consci\u00eancia \u00e9 o primeiro passo para a mudan\u00e7a. Eu apoio #hugatree #amoanatureza\n",
        "topics: ['animals              1.0']\n",
        "topic: animals\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}